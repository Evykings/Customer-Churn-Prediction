# -*- coding: utf-8 -*-
"""Predictive_Analytics_in_Customer_Relationship_Management (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LeRsmXTHEBIVpbWwz3fLtQpPWWynN_Y9
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from imblearn.over_sampling import SMOTE

"""##Data Preprocessing"""

# Set the aesthetic style of the plots
sns.set_style('whitegrid')

# Load the dataset
#data source: https://archive.ics.uci.edu/dataset/352/online+retail

#This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011
#for  a registered  online retail.

try:
    df = pd.read_csv('/content/data.csv', encoding='utf-8')  # default
except UnicodeDecodeError:
    try:
        df = pd.read_csv('/content/data.csv', encoding='ISO-8859-1')  # Latin1
    except UnicodeDecodeError:
        df = pd.read_csv('/content/data.csv', encoding='cp1252')  # Windows





"""##Preliminary Data Checks"""

# Display the first few rows of the dataframe
df.head()

# General information about data types and missing values
df.info()

# Summary statistics for numerical features
df.describe()

"""## Data Cleaning"""

# Convert 'InvoiceDate' to datetime format
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

# Checking for and handling missing values
df.isnull().sum().plot(kind='bar', title='Missing Values Per Column')
plt.show()

# Droping rows having missing values

df = df.dropna()
df.shape

# Changing the datatype of Customer ID

df['CustomerID'] = df['CustomerID'].astype(str)

# New Attribute : Monetary

df['Amount'] = df['Quantity']*df['UnitPrice']
rfm_m = df.groupby('CustomerID')['Amount'].sum()
rfm_m = rfm_m.reset_index()
rfm_m.head()

# New Attribute : Frequency

rfm_f = df.groupby('CustomerID')['InvoiceNo'].count()
rfm_f = rfm_f.reset_index()
rfm_f.columns = ['CustomerID', 'Frequency']
rfm_f.head()

# Merging the two dfs

rfm = pd.merge(rfm_m, rfm_f, on='CustomerID', how='inner')
rfm.head()

# New Attribute : Recency

# Convert to datetime to proper datatype

df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'],format='%d-%m-%Y %H:%M')

# Compute the maximum date to know the last transaction date

max_date = max(df['InvoiceDate'])
max_date

# Compute the difference between max date and transaction date

df['Diff'] = max_date - df['InvoiceDate']
df.head()

# Compute last transaction date to get the recency of customers

rfm_p = df.groupby('CustomerID')['Diff'].min()
rfm_p = rfm_p.reset_index()
rfm_p.head()

# Extract number of days only

rfm_p['Diff'] = rfm_p['Diff'].dt.days
rfm_p.head()

# Merge tha dataframes to get the final RFM dataframe

rfm = pd.merge(rfm, rfm_p, on='CustomerID', how='inner')
rfm.columns = ['CustomerID', 'Amount', 'Frequency', 'Recency']
rfm.head()

# Outlier Analysis of Amount Frequency and Recency

attributes = ['Amount','Frequency','Recency']
plt.rcParams['figure.figsize'] = [10,8]
sns.boxplot(data = rfm[attributes], orient="v", palette="Set2" ,whis=1.5,saturation=1, width=0.7)
plt.title("Outliers Variable Distribution", fontsize = 14, fontweight = 'bold')
plt.ylabel("Range", fontweight = 'bold')
plt.xlabel("Attributes", fontweight = 'bold')

"""There are 2 types of outliers and we will treat outliers as it can skew our dataset
Statistical
Domain specific
"""

# Removing (statistical) outliers for Amount
Q1 = rfm.Amount.quantile(0.05)
Q3 = rfm.Amount.quantile(0.95)
IQR = Q3 - Q1
rfm = rfm[(rfm.Amount >= Q1 - 1.5*IQR) & (rfm.Amount <= Q3 + 1.5*IQR)]

# Removing (statistical) outliers for Recency
Q1 = rfm.Recency.quantile(0.05)
Q3 = rfm.Recency.quantile(0.95)
IQR = Q3 - Q1
rfm = rfm[(rfm.Recency >= Q1 - 1.5*IQR) & (rfm.Recency <= Q3 + 1.5*IQR)]

# Removing (statistical) outliers for Frequency
Q1 = rfm.Frequency.quantile(0.05)
Q3 = rfm.Frequency.quantile(0.95)
IQR = Q3 - Q1
rfm = rfm[(rfm.Frequency >= Q1 - 1.5*IQR) & (rfm.Frequency <= Q3 + 1.5*IQR)]

"""Rescaling the Attributes¶
It is extremely important to rescale the variables so that they have a comparable scale.| There are two common ways of rescaling:

Min-Max scaling
Standardisation (mean-0, sigma-1)
Here, we will use Standardisation Scaling.
"""

# import required libraries for clustering
import sklearn
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import linkage
from scipy.cluster.hierarchy import dendrogram
from scipy.cluster.hierarchy import cut_tree

# Rescaling the attributes

rfm_df = rfm[['Amount', 'Frequency', 'Recency']]

# Instantiate
scaler = StandardScaler()

# fit_transform
rfm_df_scaled = scaler.fit_transform(rfm_df)
rfm_df_scaled.shape

rfm_df_scaled = pd.DataFrame(rfm_df_scaled)
rfm_df_scaled.columns = ['Amount', 'Frequency', 'Recency']
rfm_df_scaled.head()

"""Step 4 : Building the Model¶
K-Means Clustering
K-means clustering is one of the simplest and popular unsupervised machine learning algorithms.

The algorithm works as follows:

First we initialize k points, called means, randomly.
We categorize each item to its closest mean and we update the mean’s coordinates, which are the averages of the items categorized in that mean so far.
We repeat the process for a given number of iterations and at the end, we have our clusters.
"""

# k-means with some arbitrary k

kmeans = KMeans(n_clusters=4, max_iter=50)
kmeans.fit(rfm_df_scaled)

kmeans.labels_

"""Finding the Optimal Number of Clusters
Elbow Curve to get the right number of Clusters
A fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which the data may be clustered. The Elbow Method is one of the most popular methods to determine this optimal value of k.
"""

# Elbow-curve/SSD

ssd = []
range_n_clusters = [2, 3, 4, 5, 6, 7, 8]
for num_clusters in range_n_clusters:
    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)
    kmeans.fit(rfm_df_scaled)

    ssd.append(kmeans.inertia_)

# plot the SSDs for each n_clusters
plt.plot(ssd)

# Silhouette analysis
range_n_clusters = [2, 3, 4, 5, 6, 7, 8]

for num_clusters in range_n_clusters:

    # intialise kmeans
    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)
    kmeans.fit(rfm_df_scaled)

    cluster_labels = kmeans.labels_

    # silhouette score
    silhouette_avg = silhouette_score(rfm_df_scaled, cluster_labels)
    print("For n_clusters={0}, the silhouette score is {1}".format(num_clusters, silhouette_avg))

# Final model with k=3
kmeans = KMeans(n_clusters=3, max_iter=50)
kmeans.fit(rfm_df_scaled)

kmeans.labels_

# assign the label
rfm['Cluster_Id'] = kmeans.labels_
rfm.head()

# Box plot to visualize Cluster Id vs Frequency

sns.boxplot(x='Cluster_Id', y='Amount', data=rfm)

# Box plot to visualize Cluster Id vs Frequency

sns.boxplot(x='Cluster_Id', y='Frequency', data=rfm)

# Box plot to visualize Cluster Id vs Recency

sns.boxplot(x='Cluster_Id', y='Recency', data=rfm)

# Complete linkage

mergings = linkage(rfm_df_scaled, method="complete", metric='euclidean')
dendrogram(mergings)
plt.show()

# 3 clusters
cluster_labels = cut_tree(mergings, n_clusters=3).reshape(-1, )
cluster_labels

# Assign cluster labels

rfm['Cluster_Labels'] = cluster_labels
rfm.head()

# Plot Cluster Id vs Amount

sns.boxplot(x='Cluster_Labels', y='Amount', data=rfm)

# Plot Cluster Id vs Frequency

sns.boxplot(x='Cluster_Labels', y='Frequency', data=rfm)

# Plot Cluster Id vs Recency

sns.boxplot(x='Cluster_Labels', y='Recency', data=rfm)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from imblearn.over_sampling import SMOTE

# Assuming churn is based on recency, you could create a churn variable
rfm['Churn'] = (rfm['Recency'] > rfm['Recency'].quantile(0.75)).astype(int)

# Split the data into training and testing sets with a larger test size for more challenging predictions
X = rfm[['Amount', 'Frequency', 'Recency']]
y = rfm['Churn']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)

# Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Perform hyperparameter tuning with GridSearchCV
param_grid = {
    'C': [0.1, 1, 10, 100],
    'penalty': ['l1'],  # L2 regularization
    'solver': ['liblinear']  # Suitable for small datasets and L2 regularization
}
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_smote, y_train_smote)

# Use the best model found by GridSearchCV
best_log_reg = grid_search.best_estimator_

# Perform k-fold cross-validation
cv_scores = cross_val_score(best_log_reg, X_train_smote, y_train_smote, cv=5, scoring='accuracy')
print(f'Cross-validated accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}')

# Train the best logistic regression model
best_log_reg.fit(X_train_smote, y_train_smote)

# Make predictions
y_pred = best_log_reg.predict(X_test)

# Evaluate the model
print('Accuracy:', accuracy_score(y_test, y_pred))
print('Precision:', precision_score(y_test, y_pred))
print('Recall:', recall_score(y_test, y_pred))
print('F1 Score:', f1_score(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.show()

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming churn is based on recency
rfm['Churn'] = (rfm['Recency'] > rfm['Recency'].quantile(0.75)).astype(int)

# Split the data into training and testing sets
X = rfm[['Amount', 'Frequency', 'Recency']]
y = rfm['Churn']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Initialize and train Logistic Regression with higher regularization
log_reg = LogisticRegression(penalty='l2', C=0.1, random_state=42)  # Stronger regularization
log_reg.fit(X_train_smote, y_train_smote)
y_pred_log = log_reg.predict(X_test)

# Initialize and train a simplified Random Forest
rf_model = RandomForestClassifier(max_depth=3, n_estimators=50, min_samples_leaf=10, random_state=42)
rf_model.fit(X_train_smote, y_train_smote)
y_pred_rf = rf_model.predict(X_test)

# Initialize and train a simplified Gradient Boosting model
gb_model = GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42)
gb_model.fit(X_train_smote, y_train_smote)
y_pred_gb = gb_model.predict(X_test)

# Evaluate Logistic Regression
print('Logistic Regression:')
print('Accuracy:', accuracy_score(y_test, y_pred_log))
print('Precision:', precision_score(y_test, y_pred_log))
print('Recall:', recall_score(y_test, y_pred_log))
print('F1 Score:', f1_score(y_test, y_pred_log))

# Evaluate Random Forest
print('Random Forest:')
print('Accuracy:', accuracy_score(y_test, y_pred_rf))
print('Precision:', precision_score(y_test, y_pred_rf))
print('Recall:', recall_score(y_test, y_pred_rf))
print('F1 Score:', f1_score(y_test, y_pred_rf))

# Evaluate Gradient Boosting
print('Gradient Boosting:')
print('Accuracy:', accuracy_score(y_test, y_pred_gb))
print('Precision:', precision_score(y_test, y_pred_gb))
print('Recall:', recall_score(y_test, y_pred_gb))
print('F1 Score:', f1_score(y_test, y_pred_gb))

# Confusion Matrix for Logistic Regression
cm_log = confusion_matrix(y_test, y_pred_log)
sns.heatmap(cm_log, annot=True, fmt='d', cmap='Blues')
plt.show()

from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression

# Define the parameter grid
param_grid = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear']  # Solver that supports L1 and L2 regularization
}

# Initialize the model
log_reg = LogisticRegression()

# Initialize GridSearchCV
grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)

# Fit GridSearchCV
grid_search.fit(X_train_smote, y_train_smote)

# Best parameters and score
print("Best parameters found: ", grid_search.best_params_)
print("Best cross-validated accuracy: {:.4f}".format(grid_search.best_score_))

# Evaluate on test set
best_log_reg = grid_search.best_estimator_
y_pred = best_log_reg.predict(X_test)

# Metrics
print('Accuracy:', accuracy_score(y_test, y_pred))
print('Precision:', precision_score(y_test, y_pred))
print('Recall:', recall_score(y_test, y_pred))
print('F1 Score:', f1_score(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.show()

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# Random Forest with limited complexity
rf = RandomForestClassifier(max_depth=5, n_estimators=50, min_samples_split=10, random_state=42)
rf.fit(X_train_smote, y_train_smote)
rf_scores = cross_val_score(rf, X_train_smote, y_train_smote, cv=5)
print("Simplified Random Forest Cross-validated accuracy: {:.4f} ± {:.4f}".format(rf_scores.mean(), rf_scores.std()))

# Gradient Boosting with limited complexity
gb = GradientBoostingClassifier(max_depth=3, n_estimators=50, random_state=42)
gb.fit(X_train_smote, y_train_smote)
gb_scores = cross_val_score(gb, X_train_smote, y_train_smote, cv=5)
print("Simplified Gradient Boosting Cross-validated accuracy: {:.4f} ± {:.4f}".format(gb_scores.mean(), gb_scores.std()))

# Evaluate on test set
y_pred_rf = rf.predict(X_test)
y_pred_gb = gb.predict(X_test)

# Metrics for Simplified Random Forest
print('Simplified Random Forest Accuracy:', accuracy_score(y_test, y_pred_rf))
print('Simplified Random Forest Precision:', precision_score(y_test, y_pred_rf))
print('Simplified Random Forest Recall:', recall_score(y_test, y_pred_rf))
print('Simplified Random Forest F1 Score:', f1_score(y_test, y_pred_rf))

# Metrics for Simplified Gradient Boosting
print('Simplified Gradient Boosting Accuracy:', accuracy_score(y_test, y_pred_gb))
print('Simplified Gradient Boosting Precision:', precision_score(y_test, y_pred_gb))
print('Simplified Gradient Boosting Recall:', recall_score(y_test, y_pred_gb))
print('Simplified Gradient Boosting F1 Score:', f1_score(y_test, y_pred_gb))

from sklearn.ensemble import GradientBoostingClassifier

# Further simplified Gradient Boosting with even stronger regularization
gb_strong = GradientBoostingClassifier(max_depth=2, n_estimators=10, learning_rate=0.01, random_state=42)
gb_strong.fit(X_train_smote, y_train_smote)
gb_strong_scores = cross_val_score(gb_strong, X_train_smote, y_train_smote, cv=5)
print("Strongly Regularized Gradient Boosting Cross-validated accuracy: {:.4f} ± {:.4f}".format(gb_strong_scores.mean(), gb_strong_scores.std()))

# Evaluate on test set
y_pred_gb_strong = gb_strong.predict(X_test)
print('Strongly Regularized Gradient Boosting Accuracy:', accuracy_score(y_test, y_pred_gb_strong))
print('Strongly Regularized Gradient Boosting Precision:', precision_score(y_test, y_pred_gb_strong))
print('Strongly Regularized Gradient Boosting Recall:', recall_score(y_test, y_pred_gb_strong))
print('Strongly Regularized Gradient Boosting F1 Score:', f1_score(y_test, y_pred_gb_strong))

from sklearn.tree import DecisionTreeClassifier, plot_tree

# Train a decision tree classifier
tree_clf = DecisionTreeClassifier(max_depth=5, random_state=42)
tree_clf.fit(X_train_smote, y_train_smote)

# Make predictions
y_pred_tree = tree_clf.predict(X_test)

# Evaluate the model
print('Accuracy:', accuracy_score(y_test, y_pred_tree))
print('Precision:', precision_score(y_test, y_pred_tree))
print('Recall:', recall_score(y_test, y_pred_tree))
print('F1 Score:', f1_score(y_test, y_pred_tree))

# Plot the tree
plt.figure(figsize=(20,10))
plot_tree(tree_clf, filled=True, feature_names=['Amount', 'Frequency', 'Recency'], class_names=['No Churn', 'Churn'])
plt.show()

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Build the neural network model
model = Sequential()
model.add(Dense(64, input_dim=3, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))  # Binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_smote, y_train_smote, epochs=100, batch_size=32, validation_data=(X_test, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print('Test Accuracy:', accuracy)

# Plot accuracy and loss over epochs
plt.plot(history.history['accuracy'], label='train accuracy')
plt.plot(history.history['val_accuracy'], label='val accuracy')
plt.legend()
plt.show()

from sklearn.svm import SVC

# Train the SVM model
svm_clf = SVC(kernel='rbf', random_state=42)
svm_clf.fit(X_train_smote, y_train_smote)

# Make predictions
y_pred_svm = svm_clf.predict(X_test)

# Evaluate the model
print('Accuracy:', accuracy_score(y_test, y_pred_svm))
print('Precision:', precision_score(y_test, y_pred_svm))
print('Recall:', recall_score(y_test, y_pred_svm))
print('F1 Score:', f1_score(y_test, y_pred_svm))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred_svm)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.show()

"""K-fold cross-validation for Decision Trees and Neural Networks"""

from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier

# Assuming you have a feature matrix X and target vector y
tree_model = DecisionTreeClassifier()

# Perform k-fold cross-validation with 10 folds
cv_scores = cross_val_score(tree_model, X, y, cv=10, scoring='accuracy')

print(f"Cross-validated accuracy: {cv_scores.mean()} ± {cv_scores.std()}")

from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier

# Initialize the Decision Tree model
dt = DecisionTreeClassifier(random_state=42)

# Set up the hyperparameter grid
param_grid = {
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=dt, param_grid=param_grid,
                           cv=5, n_jobs=-1, verbose=1, scoring='accuracy')

# Fit the model
grid_search.fit(X_train, y_train)

# Best parameters and the corresponding score
print(f"Best parameters: {grid_search.best_params_}")
print(f"Cross-validated accuracy: {grid_search.best_score_:.4f}")

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report

# Define a binary target: 1 if Recency > threshold, else 0
threshold = customer_history_df['Recency'].quantile(0.75)  # example threshold
customer_history_df['Churn'] = (customer_history_df['Recency'] > threshold).astype(int)

# Features and target variable
X_logreg = customer_history_df[['Recency_log', 'Frequency_log', 'Amount_log']]
y_logreg = customer_history_df['Churn']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_logreg, y_logreg, test_size=0.3, random_state=42)

# Initialize and train the logistic regression model
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

# Make predictions
y_pred = log_reg.predict(X_test)

# Evaluate the model
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.tree import DecisionTreeClassifier, plot_tree

# Features and target
X_tree = customer_history_df[['Recency_log', 'Frequency_log', 'Amount_log']]
y_tree = customer_history_df['Churn']  # Using churn as an example

# Train the decision tree classifier
tree_clf = DecisionTreeClassifier(max_depth=4, random_state=42)
tree_clf.fit(X_tree, y_tree)

# Plot the tree
plt.figure(figsize=(20,10))
plot_tree(tree_clf, filled=True, feature_names=['Recency_log', 'Frequency_log', 'Amount_log'], class_names=['No Churn', 'Churn'])
plt.show()

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Define the neural network model
model = Sequential()
model.add(Dense(32, input_dim=3, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))  # For binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_tree, y_tree, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print('Accuracy:', accuracy)

from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score

# Train the SVM model
svm_clf = SVC(kernel='rbf', random_state=42)
svm_clf.fit(X_tree, y_tree)

# Predict and evaluate
y_pred = svm_clf.predict(X_test)
print(classification_report(y_test, y_pred))
print('Accuracy:', accuracy_score(y_test, y_pred))

# Handling missing values
df.dropna(subset=['CustomerID'], inplace=True)  # Removing rows where 'CustomerID' is missing
df['UnitPrice'].fillna(df['UnitPrice'].mean(), inplace=True)  # Imputing 'UnitPrice' with the mean

# Removing duplicates
df.drop_duplicates(inplace=True)

# Checking for missing values after handling the values
df.isnull().sum().plot(kind='bar', title='Missing Values Per Column')
plt.show()

# Parsing InvoiceNo for cancellations and creating a new column
df['IsCancelled'] = df['InvoiceNo'].apply(lambda x: 1 if str(x).startswith('C') else 0)

"""##Exploratory Data Analysis (EDA)"""

# Histogram of UnitPrice and Quantity
df[['UnitPrice', 'Quantity']].hist(bins=50, figsize=(12, 5), layout=(1, 2))
plt.show()

# Boxplot to check for outliers
plt.figure(figsize=(10, 4))
sns.boxplot(x='UnitPrice', data=df)
plt.title('Boxplot of UnitPrice')
plt.show()

# Trend analysis over time
df.set_index('InvoiceDate')['UnitPrice'].plot(figsize=(10, 5))
plt.title('Unit Price Trends Over Time')
plt.ylabel('Unit Price')
plt.show()

# Customer segmentation analysis by country
plt.figure(figsize=(10, 6))
sns.countplot(y='Country', data=df, order=df['Country'].value_counts().index)
plt.title('Customer Distribution by Country')
plt.show()

# Resampling the data to monthly frequency to see trends
# Create the 'TotalPrice' column by multiplying 'Quantity' and 'UnitPrice'
df['TotalPrice'] = df['Quantity'] * df['UnitPrice']

monthly_sales = df.set_index('InvoiceDate').resample('M')['TotalPrice'].sum()
monthly_sales.plot(figsize=(10, 4), title='Monthly Sales Over Time')
plt.ylabel('Total Sales')
plt.show()

"""##Customer Segmentation Analysis"""

# Number of transactions per customer
customer_transactions = df.groupby('CustomerID').size()
customer_transactions.hist(bins=50)
plt.title('Distribution of Transactions per Customer')
plt.xlabel('Number of Transactions')
plt.ylabel('Number of Customers')
plt.show()

# Average purchase quantity per country
country_purchases = df.groupby('Country')['Quantity'].mean().sort_values(ascending=False)
plt.figure(figsize=(10, 8))
country_purchases.plot(kind='bar')
plt.title('Average Purchase Quantity by Country')
plt.ylabel('Average Quantity')
plt.show()

"""###Relationship Between Variables


"""

# Correlation heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(df[['Quantity', 'UnitPrice', 'TotalPrice']].corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

# Scatter plot of Quantity vs UnitPrice
plt.figure(figsize=(8, 6))
sns.scatterplot(x='UnitPrice', y='Quantity', data=df)
plt.title('Scatter Plot of Quantity vs. UnitPrice')
plt.xlabel('Unit Price')
plt.ylabel('Quantity')
plt.show()

# Adding more time-based features
df['Month'] = df['InvoiceDate'].dt.month
df['Day'] = df['InvoiceDate'].dt.day
df['Weekday'] = df['InvoiceDate'].dt.weekday
df['Hour'] = df['InvoiceDate'].dt.hour
df['IsWeekend'] = df['Weekday'].isin([5, 6]).astype(int)

#Aggregate Customer Metrics
# Group by 'CustomerID' to calculate customer-based features
customer_features = df.groupby('CustomerID').agg(
    Total_Spend=('TotalPrice', 'sum'),
    Average_Spend=('TotalPrice', 'mean'),
    Total_Purchases=('InvoiceNo', 'nunique'),  # Assuming each invoice no is a separate purchase
    Frequency=('InvoiceDate', 'count')
)

# Merge these features back into the main dataframe
df = df.merge(customer_features, on='CustomerID', how='left')

# Group by 'StockCode' to calculate product-based features
product_features = df.groupby('StockCode').agg(
    Total_Sales=('TotalPrice', 'sum'),
    Sale_Frequency=('InvoiceNo', 'count')
)

# Merge these features back into the main dataframe
df = df.merge(product_features, on='StockCode', how='left')

# Dropping 'Description' as it typically requires advanced NLP techniques to be useful
X.drop('Description', axis=1, inplace=True)

#One-Hot Encoding
X = pd.get_dummies(X, columns=['Country'])

#For StockCode

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
X['StockCode'] = label_encoder.fit_transform(X['StockCode'])

# Display data types of all columns to identify non-numeric types
# Check data types
print(X.dtypes)

# Check for any remaining NaN values
print(X.isna().sum())

# Print unique values of columns suspected to still contain string data
print(X_train['InvoiceNo'].unique()[:5])  # Assuming InvoiceNo might still be included and problematic

# Assuming 'IsCancelled' is your target variable
y = df['IsCancelled']
X = df.drop('IsCancelled', axis=1)  # Ensure all other unnecessary columns are also dropped if any

X.drop(['InvoiceNo', 'StockCode', 'Description'], axis=1, inplace=True)

X = pd.get_dummies(X, columns=['Country'])

print(X.dtypes)  # Should show no object types remaining

# Extracting components from 'InvoiceDate'
X['Year'] = X['InvoiceDate'].dt.year
X['Month'] = X['InvoiceDate'].dt.month
X['Day'] = X['InvoiceDate'].dt.day
X['Hour'] = X['InvoiceDate'].dt.hour

# Now you can drop the original 'InvoiceDate' column
X.drop('InvoiceDate', axis=1, inplace=True)

# Confirm all data types
print(X.dtypes)

from imblearn.over_sampling import SMOTE

# Applying SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import train_test_split

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)

# Initialize and train the RandomForest classifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Predictions and evaluations
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X_resampled, y_resampled, cv=5, scoring='accuracy')
print("Cross-validated scores:", scores)

print("Distribution in resampled data:", np.bincount(y_resampled))

model = RandomForestClassifier(random_state=42, n_estimators=10, max_depth=5)
model.fit(X_train, y_train)
print(cross_val_score(model, X_resampled, y_resampled, cv=5, scoring='accuracy'))

from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier

# Scaling the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

# Re-apply Logistic Regression with scaled data
lr_model = LogisticRegression(max_iter=1000)
lr_scores = cross_val_score(lr_model, X_scaled, y_resampled, cv=5, scoring='accuracy')
print("Logistic Regression CV Scores on Scaled Data:", lr_scores)

# Decision Tree
dt_model = DecisionTreeClassifier(max_depth=10)
dt_scores = cross_val_score(dt_model, X_resampled, y_resampled, cv=5, scoring='accuracy')
print("Decision Tree CV Scores:", dt_scores)

# Example: Dropping one of the highly correlated features
X_resampled.drop('Average_Spend', axis=1, inplace=True)  # Keep 'Total_Spend' instead

from sklearn.decomposition import PCA

# Apply PCA to reduce dimensionality
pca = PCA(n_components=0.95)  # Retain 95% of variance
X_resampled_pca = pca.fit_transform(X_resampled)

# Re-run cross-validation on a simplified dataset
model = RandomForestClassifier(random_state=42)
scores = cross_val_score(model, X_resampled, y_resampled, cv=5, scoring='accuracy')
print("RandomForest CV Scores After Addressing Multicollinearity:", scores)

from sklearn.utils import shuffle

# Shuffle the labels and re-evaluate
y_shuffled = shuffle(y_resampled, random_state=42)
shuffled_scores = cross_val_score(model, X_resampled, y_shuffled, cv=5, scoring='accuracy')
print("Performance on Shuffled Labels:", shuffled_scores)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

# Test with Logistic Regression
lr_model = LogisticRegression(max_iter=1000, random_state=42)
lr_scores = cross_val_score(lr_model, X_resampled, y_resampled, cv=5, scoring='accuracy')
print("Logistic Regression CV Scores:", lr_scores)

# Test with a Decision Tree
dt_model = DecisionTreeClassifier(max_depth=5, random_state=42)
dt_scores = cross_val_score(dt_model, X_resampled, y_resampled, cv=5, scoring='accuracy')
print("Decision Tree CV Scores:", dt_scores)

import matplotlib.pyplot as plt

model.fit(X_resampled, y_resampled)
importances = model.feature_importances_
sorted_indices = np.argsort(importances)[::-1]

plt.figure(figsize=(12, 6))
plt.barh(range(len(importances)), importances[sorted_indices], align='center')
plt.yticks(range(len(importances)), [X.columns[i] for i in sorted_indices])
plt.xlabel('Feature Importance')
plt.title('Feature Importance after Multicollinearity Reduction')
plt.show()

from sklearn.utils import shuffle

# Shuffle labels
y_shuffled = shuffle(y_resampled, random_state=42)
shuffled_scores = cross_val_score(model, X_resampled, y_shuffled, cv=5, scoring='accuracy')
print("Performance on Shuffled Labels:", shuffled_scores)

# from sklearn.linear_model import LogisticRegression
# from sklearn.tree import DecisionTreeClassifier

# # Logistic Regression
# lr_model = LogisticRegression(max_iter=1000)
# lr_scores = cross_val_score(lr_model, X_resampled, y_resampled, cv=5, scoring='accuracy')
# print("Logistic Regression CV Scores:", lr_scores)

# # Decision Tree
# dt_model = DecisionTreeClassifier(max_depth=10)
# dt_scores = cross_val_score(dt_model, X_resampled, y_resampled, cv=5, scoring='accuracy')
# print("Decision Tree CV Scores:", dt_scores)

from sklearn.linear_model import LogisticRegression

simple_model = LogisticRegression()
simple_model.fit(X_train_smote, y_train_smote)
print("Simple Model Score:", cross_val_score(simple_model, X_resampled, y_resampled, cv=5, scoring='accuracy'))

# Dropping the 'InvoiceNo' column from both training and testing datasets
X_train.drop('InvoiceNo', axis=1, inplace=True)
X_test.drop('InvoiceNo', axis=1, inplace=True)

# Print the data types of all columns to identify non-numeric columns
print(X_train.dtypes)

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

"""#Handling Class Imbalance"""

from imblearn.over_sampling import SMOTE

# Prepare features and target
X = df.drop(['IsCancelled', 'InvoiceNo', 'InvoiceDate'], axis=1)  # Drop non-features
y = df['IsCancelled']  # Target variable

# Apply SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)

# Train the RandomForest model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Predict and evaluate
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

"""##Model Implementation

###Linear Regression
"""

import seaborn as sns

# Check the distribution of the target variable
sns.countplot(x='IsCancelled', data=df)
plt.title('Distribution of Target Variable')
plt.show()

print(df['IsCancelled'].value_counts(normalize=True))

import pandas as pd
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split

# Assuming df is your DataFrame
df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])

# Creating new time features from InvoiceDate
df['Hour'] = df['InvoiceDate'].dt.hour
df['DayOfWeek'] = df['InvoiceDate'].dt.dayofweek
df['Month'] = df['InvoiceDate'].dt.month

# One-hot encoding the 'Country' column
ohe = OneHotEncoder()
encoded_country = ohe.fit_transform(df[['Country']]).toarray()
encoded_country_df = pd.DataFrame(encoded_country, columns=ohe.categories_[0])
df = pd.concat([df, encoded_country_df], axis=1)

# Preparing X and y
features = ['Quantity', 'UnitPrice', 'Hour', 'DayOfWeek', 'Month'] + list(ohe.categories_[0])
X = df[features]
y = df['IsCancelled']  # Make sure 'IsCancelled' is in your DataFrame

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Check the number of NaN values in the target variable
print("NaN values in y_train:", y_train.isna().sum())

# Remove rows where the target variable is NaN
X_train = X_train[y_train.notna()]
y_train = y_train[y_train.notna()]

# Optionally, check and handle NaNs in features as well
print("NaN values in X_train:", X_train.isna().sum())
X_train = X_train.dropna()



# Ensuring that y_train has no NaN values
non_na_indices = y_train.notna()

# Applying the same filter to both X_train and y_train simultaneously
X_train = X_train[non_na_indices]
y_train = y_train[non_na_indices]

# Drop columns that might not be useful
X_train = X_train.drop(['InvoiceNo', 'Description'], axis=1)
X_test = X_test.drop(['InvoiceNo', 'Description'], axis=1)

# Apply one-hot encoding to 'Country' and possibly 'StockCode'
X_train = pd.get_dummies(X_train, columns=['Country', 'StockCode'])
X_test = pd.get_dummies(X_test, columns=['Country', 'StockCode'])

# It's crucial that X_train and X_test have the same features after encoding
# Ensure that both dataframes have the same dummy variables
X_train, X_test = X_train.align(X_test, join='outer', axis=1, fill_value=0)  # fill missing with 0

# Print data types and look for any objects or strings
print(X_train.dtypes)

# Extract numerical features from 'InvoiceDate'
X_train['Year'] = X_train['InvoiceDate'].dt.year
X_train['Month'] = X_train['InvoiceDate'].dt.month
X_train['Day'] = X_train['InvoiceDate'].dt.day
X_train['Weekday'] = X_train['InvoiceDate'].dt.weekday
X_train['Hour'] = X_train['InvoiceDate'].dt.hour

X_test['Year'] = X_test['InvoiceDate'].dt.year
X_test['Month'] = X_test['InvoiceDate'].dt.month
X_test['Day'] = X_test['InvoiceDate'].dt.day
X_test['Weekday'] = X_test['InvoiceDate'].dt.weekday
X_test['Hour'] = X_test['InvoiceDate'].dt.hour

# Now drop the original 'InvoiceDate' column as it's no longer needed
X_train.drop('InvoiceDate', axis=1, inplace=True)
X_test.drop('InvoiceDate', axis=1, inplace=True)

# Print the count of NaNs for each column
print(X_train.isna().sum())

# Drop columns that have all values as NaN
X_train = X_train.dropna(axis=1, how='all')
X_test = X_test.dropna(axis=1, how='all')

# Assuming 'df' is the original DataFrame and 'Country' needs to be re-encoded properly
# Ensure that 'Country' column is properly aligned and has no missing values before encoding

# Re-apply one-hot encoding correctly
countries = pd.get_dummies(df['Country'], prefix='Country')
df = pd.concat([df.drop('Country', axis=1), countries], axis=1)

# Re-split the data if necessary
from sklearn.model_selection import train_test_split
X = df.drop('IsCancelled', axis=1)  # Assuming 'IsCancelled' is the target
y = df['IsCancelled']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Display unique values in columns that are suspected to be non-numeric to confirm their data types
print(X_train['InvoiceNo'].unique()[:5])  # Display a few unique entries
print(X_train['StockCode'].unique()[:5])  # Adjust as necessary based on your DataFrame

# Drop identifier columns from training and testing datasets
X_train.drop(['InvoiceNo', 'StockCode'], axis=1, inplace=True)
X_test.drop(['InvoiceNo', 'StockCode'], axis=1, inplace=True)

# Drop the 'Description' column as it's textual and not suitable for median imputation
X_train.drop('Description', axis=1, inplace=True)
X_test.drop('Description', axis=1, inplace=True)

# Extracting numeric columns for imputation
numeric_columns = X_train.select_dtypes(include=[np.number]).columns
datetime_columns = X_train.select_dtypes(include=['datetime64']).columns

# Separate numeric and datetime data
X_train_numeric = X_train[numeric_columns]
X_train_datetime = X_train[datetime_columns]

from sklearn.impute import SimpleImputer

# Impute missing values in numeric columns only
imputer = SimpleImputer(strategy='median')
X_train_numeric_imputed = pd.DataFrame(imputer.fit_transform(X_train_numeric), columns=numeric_columns)

# Combine back the datetime columns
X_train_imputed = pd.concat([X_train_numeric_imputed, X_train_datetime], axis=1)

# Example: Impute datetime with median (or choose another strategy)
X_train_imputed[datetime_columns] = X_train_datetime.fillna(X_train_datetime.median())

# Check for any remaining NaNs
print(X_train_imputed.isna().sum())

# Drop rows where any of the data points are missing
X_train = X_train.dropna()
y_train = y_train[X_train.index]  # Ensure y_train aligns with the cleaned X_train

# Checking how much data remains after dropping missing data
print("Remaining rows:", X_train.shape[0])

# Print columns with non-numeric data types
non_numeric_columns = X_train.select_dtypes(include=['object']).columns
print("Non-numeric columns:", non_numeric_columns)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Feature Selection
features = ['Quantity', 'UnitPrice']
target = 'TotalPrice'

X = df[features]
y = df[target]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Training
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Predictions
y_pred = linear_model.predict(X_test)

# Evaluation
print(f'Mean Squared Error: {mean_squared_error(y_test, y_pred)}')
print(f'R^2 Score: {r2_score(y_test, y_pred)}')

# Display the first few rows of the dataset to inspect data again
print(df.head())

# Check if any feature is directly or indirectly related to the target variable
print(df.describe())

# Verify if the target variable can be trivially predicted by any single feature
for feature in features:
    print(f'Correlation between {feature} and IsCancelled: {df[feature].corr(df["IsCancelled"])}')

# Verify that the features are not derived from the target
print(df.head())
print(df.info())

plt.figure(figsize=(8, 6))
sns.boxplot(x=df['UnitPrice'])
plt.title('Boxplot of UnitPrice to Handle Outliers')
plt.xlabel('UnitPrice')
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(df['TotalPrice'], bins=30, kde=True)
plt.title('Distribution of TotalPrice')
plt.xlabel('TotalPrice')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(8, 6))
sns.lineplot(x='HourOfDay', y='UnitPrice', data=df)
plt.title('UnitPrice by Hour of Day')
plt.xlabel('Hour of Day')
plt.ylabel('UnitPrice')
plt.show()

plt.figure(figsize=(8, 6))
sns.histplot(df['Log_UnitPrice'], bins=30, kde=True)
plt.title('Log-Transformed UnitPrice Distribution')
plt.xlabel('Log of UnitPrice')
plt.ylabel('Frequency')
plt.show()

"""Figure 4.14: Linear Regression Model - Example of Predictions vs. Actuals


"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np

# Example setup for Linear Regression
X = df[['TotalPrice_Scaled']]  # Example feature
y = df['Quantity']  # Example target variable

model = LinearRegression()
model.fit(X, y)
df['Predicted_Quantity'] = model.predict(X)

plt.figure(figsize=(8, 6))
plt.scatter(df['Quantity'], df['Predicted_Quantity'], alpha=0.5)
plt.plot([df['Quantity'].min(), df['Quantity'].max()],
         [df['Quantity'].min(), df['Quantity'].max()], 'k--', color='red')
plt.title('Linear Regression: Actual vs. Predicted Quantity')
plt.xlabel('Actual Quantity')
plt.ylabel('Predicted Quantity')
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Example setup for Random Forest
X = df[['TotalPrice_Scaled', 'Country_Encoded']]  # Example features
y = df['Quantity'] > 10  # Example binary target variable

model = RandomForestClassifier(n_estimators=100)
model.fit(X, y)
importances = model.feature_importances_

plt.figure(figsize=(8, 6))
sns.barplot(x=['TotalPrice_Scaled', 'Country_Encoded'], y=importances)
plt.title('Feature Importance from Random Forest Model')
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.show()

from sklearn.model_selection import cross_val_score

# Example setup for cross-validation
model = LinearRegression()
cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')

plt.figure(figsize=(8, 6))
plt.plot(range(1, 6), -cv_scores, marker='o')
plt.title('Cross-Validation: Mean Squared Error for Each Fold')
plt.xlabel('Fold')
plt.ylabel('Mean Squared Error')
plt.show()

# Check the distribution of the target variable
df['Quantity'].value_counts()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Example setup for binary classification
# Ensure y and y_pred are binary or categorical
y = (df['Quantity'] > 10).astype(int)  # Example binary target variable
y_pred = (model.predict(X) > 0.5).astype(int)

# Compute the confusion matrix
conf_matrix = confusion_matrix(y, y_pred)

# Create confusion matrix display
disp = ConfusionMatrixDisplay(conf_matrix, display_labels=['Low', 'High'])

# Plot the confusion matrix
plt.figure(figsize=(8, 6))
disp.plot(cmap='Blues')
plt.title('Confusion Matrix for Model Evaluation')
plt.show()

from sklearn.model_selection import GridSearchCV

# Example setup for hyperparameter tuning
param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X, y)

results = pd.DataFrame(grid_search.cv_results_).sort_values(by='mean_test_score', ascending=False)

plt.figure(figsize=(8, 6))
sns.lineplot(data=results, x='param_n_estimators', y='mean_test_score', hue='param_max_depth', marker='o')
plt.title('Hyperparameter Tuning Results')
plt.xlabel('Number of Estimators')
plt.ylabel('Mean Test Score')
plt.show()